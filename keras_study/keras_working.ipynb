{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Python\n",
    "Kerasa MNIS veri setini yüklüyoruz.\n",
    "Makine öğrenmesinde kategori, sınıflandırma problemlerinde sınıf olarak adlandırılır. Her bir veriye örnek denir. Belirli bir örneğin ait olduğumu sınıf etiket-label olarak adlandırılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ağ Mimarisi\n",
    "Derin ağların temel yapı taşı, veri için filtre olarak düşünülebilecek veri işleme modülü olan KATMANLAR'dır.\n",
    "\n",
    "*Dense - Fully Connected*: Önceki katman ile kendi katmanındaki tüm birimlerin birbirine tamamen bağlı olduğu katmandır.\n",
    "İkinci katman, 10 adet çıktı birimi bulunan ve 10 elemanlı olasılık puanlarını gösteren (toplamları 1) bir diziyi geriye döndürür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import  models\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Kayıp Fonksiyonu - Loos Function*: Ağımızın eğitim veri seti üzerinde kendi performansını gözlemlemesi ve böylece kendi kendine doğru yolu bulabilmesi için.\n",
    "*Eniyileme - Optimization*: Ağımızın girdisi olan veri ile oluşturduğu kaybı göz önünde bulundurarak kendisini güncelleme mekanizması.      \n",
    "*Eğitim ve test süresince takip edilecek metrikler*: Burada biz sadece doğruluğa (doğru sınıflandırılan görüntülerin toplam görüntü sayısına oranı) odaklanacağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derleme Adımı\n",
    "Eğitime başlamadan önce, tüm girdilerimizdeki değerleri [0,1] aralığına ölçeklendiriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Girdilerin Hazırlanması\n",
    "Bu aşamadan önce eğitim veri setimizdeki görüntüler (60000,28,28) şeklinde bir dizide ve her elemanı \"uint8\" veri tipinde [0,255] veri aralığında saklanmıştı.\n",
    "Eğitim veri setindeki görüntüleri (60000,28,28) \"float32\" veri tipinde 0 ile 1 arasında olacak şekilde düzenliyoruz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000,28 * 28))\n",
    "train_images = train_images.astype('float32') / 255 #Buradaki bölme işlemi 0-255 arasındaki değerli 0-1 arasında indirmek için.\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etiketlerin Hazırlanması\n",
    "Kategorik olarak etikeliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fit etmek (uydurmak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ağımızı test veri seti üzerinde deniyoruz.\n",
    "Eğitim ve test seti arasındaki doğruluk oranı aralarındaki fark *Aşırı Uydurmanın (overfitting)* örneğidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print(\"test_acc:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train_image'mizin eksen sayısını yani *ndim*'i görüntüleyelim.\n",
    "\n",
    "Sonuç olarak, 8 Bitlik tam sayılar taşıyan 3B tensörümüz var.\n",
    "Daha net konuşursak, 28x28 boyutundaki matrislerden 60000 adet bulunduran bir dizimiz var.\n",
    "Her matris 0 ile 255 arasında bir değer alan gri ölçeğinde (grey scale) bir görüntüdür.(Bunu veri setinden biliyoruz.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(\"Train Image'mizin ndim'i:\",train_images.ndim,\"\\nTrain Image shape ve dtype:\",train_images.shape,train_images.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veri setimizdeki 4'üncü örneğe bir göz atalım"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "digit=train_images[4]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy ile Tensörler üzerinde işlemler\n",
    "Bu örnek sayesinde 10 ile 100 (100 dahil değil) arasındaki örnekleri seçiyoruz. Bunlarıda bir dizi halinde kaydediyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "my_slice = train_images[10:100]\n",
    "print(my_slice.shape)\n",
    "#Daha detay bir notasyon ile aynı örneği gösterelim\n",
    "my_slice = train_images[10:100, : , : ] #Önceki ile aynı sonucu verir çünkü eksenleri seçmedik.\n",
    "my_slice = train_images[10:100, 0:28, 0:28]#Önceki ile aynı sonucu verir çünkü bütün eksen boyunca seçim yaptık.\n",
    "#Sağ alt köşeden 14x14'lük bir kısmı seçmek için:\n",
    "my_slice = train_images[ : , 14: , 14: ]\n",
    "#Ortadan 14x14'lük bir kısmı seçmek için:\n",
    "my_slice = train_images[ : , 7:-7, 7:-7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veri Yığını Notasyonu\n",
    "Veri taşıyan tensörlerin genellikle birinci ekseni *örnek ekseni*'dir. MNIST örneğimizde, örneklerimiz rakamların görüntüleridir.\n",
    "Derin öğrenme modelleri tüm verileri bir kere de kullanmak yerine, küçük *yığınlara-Batch* ayırır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch = train_images[:128]# Büyüklüğü 128 olan bir yığın.\n",
    "batch = train_images[128:256]# Bir sonraki yığın....\n",
    "# n'inci yığını gösterebilmek içinse:\n",
    "n=0 #n sıfırdan başladığını varsayıyoruz.\n",
    "batch = train_images[n*128: (n+1)*128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Veri Tensörleri için Gerçek Dünya Örnekleri\n",
    "**Vektör Verisi**: 2B Tensörler (örnekler, öznitelikler)   \n",
    "**Zaman Serisi verisi ya da ardaşık veri**: 3B Tensörler (örnekler, zaman_adımı, öznitelikler)   \n",
    "**Görüntüler**: 4B Tensörler (örnekler, yükseklik, genişlik, kanallar)   \n",
    "**Video**: 5B Tensörler (örnekler, kareler, yükseklik, genişlik, kanllar)   \n",
    "\n",
    "### Vektör Verisi\n",
    "Birinci eksen \"Örnek Ekseni\" ikinci eksen \"Öznitelik Ekseni\"dir. Gerçek Hayat Örneği;            \n",
    "Sigorta istatistikleri veri setimizde her kişi için yaş, posta kodu, ve aylık gelir bilgileri olsun.\n",
    "Her kişi için üç değerli bir vektör olursa 100000 kişi için tüm veri seti (100000, 3) olur.\n",
    "\n",
    "### Zaman Verisi ya da Ardaşık Veri\n",
    "Ne zaman veri setinde zaman ( ya da ardaşıklık durumu) işin içine girerse verileri bir ekseni zaman olacak şekilde 3B tensörlerde saklamak gerekir.\n",
    "Zaman ekseni olarak ikinci eksen( eksenler sıfırdan başlar yani eksen 1) olarak kullanımı yaygındır.Örnek olarak Hisse senedi veri seti:   \n",
    "Her dakika bir hisse senedinin anlık en düşük fiyatını ve en yüksek fiyatını saklayacağız.\n",
    "Dolayısıyla her dakika 3B bir vektör olacağından tüm günün verisi (390, 3) şeklinde bir 2B tensör ve 250 günlük veri ise (250,390,3) şeklinde bir 3B tensör olacaktır.Burada her örnek bir günün tüm verisini oluşturacaktır.\n",
    "Bir diğer Örnek ise;    \n",
    "Her bir 128 farklı karakterden birisi olan 280 karakterlik bir dizi olacak şeklinde tweet verisi olsun.\n",
    "Bu durumda her karekter 128 elemanlı bir ikilik-binary vektör olacaktır.(Tüm elemanlar sıfır, sadece ilgili karakterin indeksi 1 olacak.)\n",
    "Sonra her tweet (280, 128) şeklinde bir 2B tensör olacağından 1 milyon tweet bulunan veri setimiz (1000000, 280, 128) şeklinde bir tensör olacaktır.\n",
    "\n",
    "### Görüntü Verisi\n",
    "Görüntülerin tipik olarak 3 boyutu vardır: Yükseklik, genişlik ve kanal sayısı.\n",
    "256x256 boyutunda 128 gri ölçekli ( gri ölçekte görüntülerin sadece 1 kanalı mevcuttur.) örnekler için yığın (128, 256, 256, 1) şeklinde olur.\n",
    "3 renk derinliği olan bir görüntü içinse (128, 256, 256, 3) şeklinde tensörler olacaktır.\n",
    "Tensorflow renk derinliği eksenini en sonda kullanır. (örnekler, yükseklük, genişlik, kanallar)\n",
    "\n",
    "### Video Verisi\n",
    "Video (5B Tensörler), her biri renkli ardaşık kareler olarak düşünülebilir. (örnek, kare, yükseklik, genişlik, kanllar)\n",
    "Örneğin 60 saniyelik, 144x256 boyutunda bir Youtube videosu saniyede 4 kare ile örneklenmiş olsun (toplam 240 kare).\n",
    "Böyle 4 video bulunan bir yığın  (4, 240, 144, 256, 3) şeklinde bir tensör olacaktır.\n",
    "Toplamda 106168320 değer. Eğer veri tipi float32 olursa bu tensörün büyüklüğü 405mb olacaktır.\n",
    "Gerçek hayatta kullandığımız videolar daha küçüktür çünkü, veri tipi float32 değildir ve yüksek oranda sıkıştırılırlar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensör İşlemleri\n",
    "AND, OR, NOR vb. işlemlerin tensör verileri üzerinde uygulanması gibidir. Tensör toplamı, çarpımı vb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.layers.Dense(512, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu katmanı 2 boyutlu bir girdiyi iki boyutlu bir girdiye dönüştüren bir fonksiyon olarak düşünebiliriz.\n",
    "Böylece fonksiyon W: 2B bir tensör, b: bir vektör olmak üzere;\n",
    "`output = relu(dot(W, input) + b)`   \n",
    "Burada 3 tensör işlemi bulunmaktadır;\n",
    "1. Girdi verileri ile W'nun iç çarpımıdır.(dot)\n",
    "2. İç çarpımın sonucu ile b vektörünün toplanmasıdır.\n",
    "3. relu(max(x,0)) işlemidir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eleman Bazlı İşlemler\n",
    "**relu** ve **toplama** işlemi *eleman bazlı-element wise* işlemlerdir. Bu işlemler tensörün her elemanına ayrı ayrı uygulanırlar.\n",
    "Bu da eleman bazlı işlemleri paralel programlamaya uygun hale getirirler.\n",
    "Python'da naif bir şekilde elemaz bazlı işlemleri yapmak için relu'da olduğu gibi for döngüsü ile yapabiliriz;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def naive_relu(x): #x, 2B Numpy Tensörüdür.\n",
    "    assert len(x.shape) == 2 \n",
    "    x = x.copy() # Giren tensörün üzerine yazmamak için.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] = max(x[i,j],0)\n",
    "    return x\n",
    "#Toplama işlemi için:\n",
    "def naive_add(x, y):\n",
    "    assert len(x.shape) == 2 # x ve y 2B bir Numpy Tensörü\n",
    "    assert x.shape == y.shape\n",
    "    x = x.copy() # Giren tensörün üzerine yazmamak için.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += x[i,j]\n",
    "    return x\n",
    "# Aynı işlemlerin Numpy ile yapımı:\n",
    "z = x + y # Eleman bazlı toplama\n",
    "z = np.maximum(z,0.) # Eleman bazlı relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yayma Operasyonu\n",
    "Bir önceki naive_Add naif uygulamamız sadece aynı şekle sahip ve 2B tensörleri destekliyor.\n",
    "Daha önce Dense katmanında 2B bir tensör ile bir vektörü topladık. O zaman iki farklı şekle sahip tensör söz konusu olduğunda şöyle yapılır;\n",
    "Mümkün olduğunda ve anlam farklılığı olmadığında küçük tensör büyük tensörün şekline yayılır-broadcast. Yayma iki aşamada gerçekleşir:\n",
    "1. Küçük tensöre eksenler (yayma ekseni olarak adlandırılır) eklenerek büyük tensörün ndim ile eşit hale getirilir.\n",
    "2. Küçük tensör yeni eksen üzerinde tekrarlanarak büyük tensörün şekli ile aynı şekle getirilir.   \n",
    "\n",
    "(32,10) şeklinde bir X tensörü ve (10, ) şeklinde bir y düşünelim. Önce y'ye bir boş eksen ekleyelim ve böylece y (1,10) şeklinde olsun.\n",
    "Bu yeni eksende y'yi 32 defa tekrar edersek (32,10) şeklinde bir Y elde ederiz.\n",
    "Her `i range(0,32)` olmak üzere `Y[i,:] == y` olacaktır. Artık X ve Y aynı şekle gelmiş oldular.\n",
    "Bu tekrarlama işlemi tamamen sanal olarak yapıldı:Hafıza seviyesi yerine algoritmik olarak yapıldı.\n",
    "Çünkü vektürü yeni eksen üzerinde 10 kez tekrar etmek oldukça akılcı bir işlemdir.   \n",
    "Örnek olarak ve konuyu daha iyi kavrayabilmek için [buradaki](https://machinelearningmastery.com/broadcasting-with-numpy-arrays/) yazıyı okuyabilirsiniz.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "\n",
    "x=array([[0,1],[3,4]])\n",
    "y=array([2,3])\n",
    "\n",
    "def naive_add_matrix_and_vector(x,y):\n",
    "    assert len(x.shape) == 2 # x 2B Numpy vektörü.\n",
    "    assert len(y.shape) == 1 # y Numpy vektörü.\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    \n",
    "    x = x.copy() # Girdinin üzerine yazmamak için.\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i,j] += y[j]\n",
    "    return x\n",
    "\n",
    "print(naive_add_matrix_and_vector(x,y))\n",
    "#Eleman bazlı maximum işlemi:\n",
    "\n",
    "# x (64, 3, 32, 10) şeklinde rastgele değerli olan bir tensör.\n",
    "\"\"\"\n",
    "x = np.random.random((64, 3, 32, 10))\n",
    "y =np.random.random((32, 10)) # y (32, 10) şeklinde rastgele değerli bir tensör.\n",
    "\n",
    "z= np.maximum(x,y) # z çıktısının şekli (64,3,32,10)\n",
    "print(z.shape)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensör iç Çarpımı (Dot Product)\n",
    "İç çarpım `(dot)` işlemi aynı zamanda tensör çarpımı (eleman bazlı çarpım ile karıştırmamalıyız) olarak da anılır ve çok kullanışlı bir tensör işlemidir. Eleman bazlı işlemlere kıyasla iki girdisini birleştirir.   \n",
    "Eleman bazlı çarpım işlemi Numpy, Keras, Theano ve TensorFlow'da * operaötü ile yapılır. `dot`işlemi Tensorflow'da farklı bir söz dizimi kullanır. Ancak hem Numpy hem de Keras standart dot işlemini kullanır.   \n",
    "Matematiksel gösterini şöyledir;    \n",
    "**z = x.y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array([[0,1],[3,4]])\n",
    "y=np.array([2,3])\n",
    "z = np.dot(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matematiksel olarak iç çarpım işlemi ne yapmaktadır? Önce iki vektörün iç çarpımını yapalım:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([0,1,2])\n",
    "y=np.array([2,3,4])\n",
    "def naive_vector_dot(x,y):\n",
    "    assert len(x.shape) == 1 # x ve y Numpy vektörleri. Eğer iki olsaydı matris olurlardı.\n",
    "    assert len(y.shape) == 1 \n",
    "    assert x.shape[0] == y.shape[0] # x ve y Numpy vektörlerinin uzunlukları eşit olmak zorundadır.\n",
    "    \n",
    "    z=0.\n",
    "    for i in range(x.shape[0]):\n",
    "        z += x[i] * y[i]\n",
    "    return z\n",
    "\n",
    "print(naive_vector_dot(x,y))\n",
    "print(\"X.shape:\", len(x.shape), x.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İki vektörün iç çarpımı skalerdir ancak iki vektörün eleman sayılarının aynı olması halinde iç çarpım yapılabilmektedir.\n",
    "Aynı zamanda bir *x* matrisi ile *y* vektörünün de iç çarpımını yapmak mümkündür. Sonuçta elemanları *y* vektörü ile *x* matrisinin satırlarının çarpımından oluşan bir vektör elde edilir. Daha detaylı bilgi için [buraya](https://towardsdatascience.com/linear-algebra-basics-dot-product-and-matrix-multiplication-2a7624942810#:~:text=Dot%20products%20are%20done%20between,to%20the%20number%20of%20columns.) bakabilirsiniz. Bu iç çarpımı yapan kod şu şekildedir;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array([[0,1],[3,4]])\n",
    "y=np.array([2,3])\n",
    "def naive_matrix_vector_dot(x,y):\n",
    "    assert len(x.shape) == 2# x Numpy Matrisi\n",
    "    assert len(y.shape) == 1# y Numpy Vektörü\n",
    "    assert x.shape[1] == y.shape[0]# x matrisinin birinci boyutu ile y vektörünün sıfırıncı boyutunun aynı olması gerekir.\n",
    "    \n",
    "    z=np.zeros(x.shape[0])# Bu işlem y ile aynı şekle sahip 0'lardan oluşan bir vektör oluşturur.\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            z[i] += x[i, j] * y[j]\n",
    "    return z\n",
    "print(naive_matrix_vector_dot(x,y))\n",
    "print(np.zeros(x.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bu noktada daha önce yazdığımız ve matris vektör çarpımı ile vektör çarpımında kullandığımız kodu kullanmamız mümkündür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İki tensörden herhangi birinin *ndim* boyutunun 1'den büyük olması halinde iç çarpım simetrik olmayacaktır. Yani `dot(x,y) dot(y,x)`'den farklı olacaktır.   \n",
    "Elbette iç çarpım herhangi boyutta tensörlerle genelleştirilebilse de genellikle iki matrisin iç çarpımında uygulanmaktadır. İki matrisin iç çarpımını `(dot(x,y))` ancak `x.shape[1], y.shape[0]` olması halinde yapabiliriz. Sonuç olarak da `(x.shape[0], y.shape[1])` şeklinde olan ve elemanları *x* matrisinin satırları ile *y* matrisinin sütunlarının çarpımına eşit olan bir matris oluşacaktır. Bunun kodlaması şu şekildedir;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x,y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    # x'in 1'inci ekseni ile y'nin 0'ıncı ekseninin boyutları aynı olmalıdır.\n",
    "    assert x.shape[1] == y.shape[0]# Yani x'in sütunu y'nin satırı eşit olmalı\n",
    "    z=np.zeros((x.shape[0]), y.shape[1])# Bu işlem istenen boyutta tüm elemanları sıfır olan bir matris döndürür.\n",
    "    for i in range(x.shape[0]):# x'in satırları üzerinde döner.\n",
    "        for j in range(y.shape[1]):# y'nin sütunları üzerinde döner.\n",
    "            row_x = x[i,:]\n",
    "            column_y = y[:,j]\n",
    "            z[i,j] = naive_vector_dot(row_x,column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensör Şekil Değiştirme\n",
    "Tensör şeklini değiştirme satır ve sütunlarını ayarlayarak hedef şekle uygun hale getirme anlamına gelir. Yeniden şekillendirilmiş tensör doğal olarak önceki tensör ile aynı eleman sayısına sahip olacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([\n",
    "    [0.,1.],\n",
    "    [2.,3.],\n",
    "    [4.,5.]])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "x = x.reshape((6,1))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.reshape((2,3))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensör şekil değiştirmenin özel bir hali de *devrik-tanspose* işlemidir. Matrisin devriğinin alma satır ve sütunların yer değiştirmesi anlamına gelmektedir. Yani x[i,:], x[:,i] haline gelmektedir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((300,20))\n",
    "x = np.transpose(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensör İşlemlerinin Geometrik Gösterimi\n",
    "Tensörün elemanlarının greometrik uzayda bir nokta olduğunu düşünülürse tüm tensör işlemlerinin geometrik gösterimi mevcuttur. İlgin-affine dönüşümler, yön değiştirmeler, büyütme, küçültme gibi temel geometrik işlemler tensör işlemi olarak ifade edilebilir. Örneğin 2B bir vektörün yönünün değiştirilmesi 2x2'lik bir matris olan `R = [u,v]` ile iç çarpımı yapılarak elde edilebilir. Burada `u = [cos(theta),sin(theta)]` ve `v = [-sin(theta),cos(theta)]` olacak şekilde theta açısı kadar vektörün yönünün değiştirilmesi mümkündür."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentumlu Stokastik Gradyan İnişi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`past_velocity = 0.`   \n",
    "`momentum = 0.1 #Sabit momentum faktörü`    \n",
    "    \n",
    "`while loss > 0.01: #Eniyileme Faktörü`   \n",
    "`   w, loss, gradient = get_current_parameters()`   \n",
    "`   velocity = past_velocity * momentum +learning_rate * gradient`   \n",
    "`   w = w + momentum * velocity - learning_rate *gradient`   \n",
    "`   past_velocity = velocity`   \n",
    "`   update_parameters(w)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Film Kritiklerini Sınıflandırma: İkili Sınıflandırma Örneği\n",
    "Bu örnekte, film kritiklerinin içeriklerine bakarak olumlu, olumsuz olarak sınıflandıracağız."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBD Veri Seti\n",
    "50000 adet film kritiği içeren IMBD veri setini kullanacağız. 25 örnek eğitim veri seti olarak 25000 örnek ise test veri tabanı olarak, kullanılmak üzere ikiye ayrılmıştır. Aynı MNIST veri seti gibi IMBD veri setide Keras ile beraber gelmektedir ve önişlemden geçirilmiştir: Kritikler (kelime dizileri) her sayının sözlükte bir kelimeyi temsil eden sayı dizisine dönüştürülmüştür."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "(train_data, train_label), (test_data, test_label) = imdb.load_data(num_words=10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_words` parametresi ile en sık tekrar eden 10000 örneği saklayıp nadir örnekleri göz ardı ettik. Böylece bu yönetilebilir bir vektör verisi ile çalışma imkanı sağlayacaktır.   \n",
    "En çok kullanılan 10000 kritiği kullanacağımız için hiçbir kelime indeksi 10000'i geçmyecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kelimelerin index numarası şeklinde tutulduğu veriyi tekrar İngilizce kelimelere aşağıdaki kod ile dönüştürebiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index kelimeleri bir tam sayı indekse eşleyen sözlüktür.\n",
    "word_index=imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tam sayı indexleri kelimelere dönüştürür.\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kritikleri alır. İndeks değerlerini 3 atlayarak almaktayız. Çünkü 0,1 ve 2 sırasıyla yapılan sıfır eklemelerini, dizinin başlangıcını ve bilinmeyenleri kodlamak için ayrılmıştır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verileri Hazırlamak\n",
    "İçerisinde sayı bulunan listeleri sinir ağlarına gönderemeyiz. Listeleri tensörlere dönüştürmemiz gerekmektedir. Bunu iki şekilde yapabiliriz:   \n",
    "    1- Tüm elemanları aynı uzunlukta olacak şekilde eksikleri sıfırlar ile doluracağız ve (samples, word_indices) şeklinde tam sayı tensörlerine dönüştüreceğiz. İlk katmanımızı bu tam sayı tensörlerini kullanabilecek şekilde tasarlamamız gerekmektedir.   \n",
    "    2-Listemizi bir-eleman-bir (one hot) olarak kodlayacağız. Bu şu anlama geliyor: Diyelim elimizde [3,5] olan bir dizi olsun. Bu diziyi 10000 boyutlu 3'üncü ve 5'inci indeks değerleri 1 diğer hepsi 0 olacak şekilde bir vektör oluşturacağız. Sonrasında *Dense* katmanı gibi bir katmanı kullanabiliriz.   \n",
    "Şimdi ikinci yöntemi kullanarak verilerimizi vektör haline getireceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(train_data[0])\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    #(len(sequences),dimension) şeklinde tüm elemanları sıfır olan bir matris oluşturur.\n",
    "    results = np.zeros((len(sequences),dimension))\n",
    "    for i, sequences in enumerate(sequences):\n",
    "        results[i, sequences] = 1. # results[i]'nin istenen indekslerini 1 yapar.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data) #Eğitim vektör verisi\n",
    "x_test = vectorize_sequences(test_data) #Test vektör verisi\n",
    "y_train = np.asarray(train_label).astype('float32')\n",
    "y_test = np.asarray(test_label).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}